
import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';

const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
const supabaseKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
const openAIApiKey = Deno.env.get('OPENAI_API_KEY')!;

const supabase = createClient(supabaseUrl, supabaseKey);

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

// Generate embeddings for RAG detection
async function generateEmbedding(text: string): Promise<number[]> {
  const response = await fetch('https://api.openai.com/v1/embeddings', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${openAIApiKey}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'text-embedding-ada-002',
      input: text,
    }),
  });

  const data = await response.json();
  return data.data[0].embedding;
}

// Check if question is related to RAG content
async function checkRagRelevance(question: string): Promise<{ isRelevant: boolean; matchCount: number }> {
  try {
    const queryEmbedding = await generateEmbedding(question);
    
    const { data, error } = await supabase.rpc('search_document_chunks', {
      query_embedding: JSON.stringify(queryEmbedding),
      similarity_threshold: 0.7,
      match_count: 3
    });

    if (error) {
      console.error('Error checking RAG relevance:', error);
      return { isRelevant: false, matchCount: 0 };
    }

    return { 
      isRelevant: data && data.length > 0, 
      matchCount: data ? data.length : 0 
    };
  } catch (error) {
    console.error('Error in RAG relevance check:', error);
    return { isRelevant: false, matchCount: 0 };
  }
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { messages, ragMode } = await req.json();

    if (!messages || messages.length === 0) {
      return new Response(JSON.stringify({ 
        questions: [
          'ä¼æ¥­æœ‰å“ªäº›å¸¸è¦‹çš„æ¸›ç¢³æ–¹æ³•ï¼Ÿ',
          'å¦‚ä½•åˆ¶å®šæœ‰æ•ˆçš„æ¸›ç¢³è·¯å¾‘åœ–ï¼Ÿ',
          'å†ç”Ÿèƒ½æºæ†‘è­‰(REC)å¦‚ä½•å¹«åŠ©ä¼æ¥­æ¸›ç¢³?'
        ]
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    // Get recent conversation context (last 3-5 messages)
    const recentMessages = messages.slice(-6);
    const conversationContext = recentMessages
      .map(msg => `${msg.type}: ${msg.content}`)
      .join('\n');

    // Get the last user question for RAG relevance check
    const lastUserMessage = recentMessages
      .filter(msg => msg.type === 'user')
      .pop();

    let ragRelevance = { isRelevant: false, matchCount: 0 };
    if (lastUserMessage && !ragMode) {
      ragRelevance = await checkRagRelevance(lastUserMessage.content);
    }

    // Create context-aware prompt for question generation
    const systemPrompt = `ä½ æ˜¯ä¸€å€‹æ™ºèƒ½å•é¡Œæ¨è–¦ç³»çµ±ï¼Œå°ˆé–€ç‚ºæ¸›ç¢³æ•™è‚²å°è©±ç”Ÿæˆå¾ŒçºŒå•é¡Œã€‚

ä½ çš„ä»»å‹™æ˜¯åˆ†æå°è©±è„ˆçµ¡ï¼Œç”Ÿæˆ 4-6 å€‹ç›¸é—œçš„å¾ŒçºŒå•é¡Œï¼Œå¹«åŠ©ç”¨æˆ¶æ·±å…¥å­¸ç¿’ã€‚

å•é¡Œç”Ÿæˆç­–ç•¥ï¼š
1. **è©±é¡Œå»¶çºŒ**ï¼šåŸºæ–¼ç•¶å‰è¨è«–çš„ä¸»é¡Œï¼Œç”Ÿæˆæ›´æ·±å…¥çš„å•é¡Œ
2. **æ©«å‘æ“´å±•**ï¼šæä¾›ç›¸é—œä½†ä¸åŒè§’åº¦çš„å•é¡Œ  
3. **å¯¦å‹™æ‡‰ç”¨**ï¼šå°‡ç†è«–è½‰åŒ–ç‚ºå¯¦éš›æ‡‰ç”¨å ´æ™¯
4. **æ¼¸é€²å¼æ·±åº¦**ï¼šå¾åŸºç¤æ¦‚å¿µé€æ­¥å¼•å°åˆ°é€²éšæ‡‰ç”¨

å•é¡Œé¡å‹æ¨™è¨˜ï¼š
- ğŸŸ¢ åŸºç¤æ¦‚å¿µå•é¡Œ
- ğŸ”µ é€²éšæŠ€è¡“å•é¡Œ
- âš™ï¸ å¯¦å‹™æ‡‰ç”¨å•é¡Œ
- ğŸ“Š æ•¸æ“šåˆ†æå•é¡Œ

${ragMode ? '' : ragRelevance.isRelevant ? 
  `\n**é‡è¦**ï¼šç”¨æˆ¶çš„å•é¡Œèˆ‡å·²ä¸Šå‚³çš„æ–‡ä»¶å…§å®¹ç›¸é—œï¼ˆåŒ¹é…åº¦ï¼š${ragRelevance.matchCount}å€‹ç›¸é—œç‰‡æ®µï¼‰ï¼Œè«‹åœ¨å…¶ä¸­1-2å€‹å•é¡Œå‰åŠ ä¸Š"ğŸ’¡å»ºè­°åˆ‡æ›åˆ°æ–‡ä»¶æ¨¡å¼ï¼š"å‰ç¶´ï¼Œä¸¦ç”Ÿæˆé‡å°æ–‡ä»¶å…§å®¹çš„æ·±åº¦å•é¡Œã€‚` : 
  ''
}

è«‹åˆ†æå°è©±å…§å®¹ï¼Œç†è§£ç”¨æˆ¶çš„å­¸ç¿’é€²ç¨‹å’Œé—œæ³¨é»ï¼Œç”Ÿæˆæ—¢ç›¸é—œåˆèƒ½å¼•å°æ·±å…¥å­¸ç¿’çš„å•é¡Œã€‚
å›å‚³æ ¼å¼ï¼š{ "questions": ["å•é¡Œä¸€", "å•é¡ŒäºŒ", ...] }`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openAIApiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: `å°è©±è„ˆçµ¡ï¼š\n${conversationContext}\n\nè«‹åŸºæ–¼æ­¤å°è©±ç”Ÿæˆæ™ºèƒ½å¾ŒçºŒå•é¡Œã€‚` }
        ],
        temperature: 0.7,
        response_format: { type: "json_object" },
      }),
    });

    if (!response.ok) {
      throw new Error(`OpenAI API request failed with status ${response.status}`);
    }

    const data = await response.json();
    const jsonResponse = JSON.parse(data.choices[0].message.content);

    return new Response(JSON.stringify({ 
      questions: jsonResponse.questions || [
        'ä¼æ¥­æ¸›ç¢³æœ‰å“ªäº›å…·é«”çš„å¯¦æ–½æ­¥é©Ÿï¼Ÿ',
        'å¦‚ä½•è©•ä¼°æ¸›ç¢³æªæ–½çš„æ•ˆç›Šï¼Ÿ',
        'æ¸›ç¢³éç¨‹ä¸­å¸¸é‡åˆ°å“ªäº›æŒ‘æˆ°ï¼Ÿ'
      ]
    }), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });

  } catch (error) {
    console.error('Error generating contextual questions:', error);
    return new Response(JSON.stringify({ 
      questions: [
        'ä¼æ¥­æ¸›ç¢³ç­–ç•¥å¦‚ä½•åˆ¶å®šï¼Ÿ',
        'ç¢³è¶³è·¡è¨ˆç®—çš„é—œéµè¦ç´ æ˜¯ä»€éº¼ï¼Ÿ',
        'ç¶ è‰²è½‰å‹å°ä¼æ¥­çš„å½±éŸ¿æœ‰å“ªäº›ï¼Ÿ'
      ]
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });
  }
});
